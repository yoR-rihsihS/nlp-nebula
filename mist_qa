import os
import subprocess
import sys
import time


os.environ["PATH"] += "/data/home1/shishirm/.local/lib/python3.8/site-packages" + os.pathsep

def update_package(package_name):
    subprocess.check_call(['pip', 'install', '--upgrade', package_name])

# update_package("nvidia-cuda-nvcc-cu11")

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# install("packaging")

# install("torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
# subprocess.check_call(['pip', 'install', 'torch', 'torchvision', 'torchaudio', '--index-url=https://download.pytorch.org/whl/cu118'])


# subprocess.check_call(['pip', 'install', 'flash-attn', '--no-build-isolation'])


from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from copy import deepcopy
import json
from typing import List, Optional, Tuple, Type, TypeVar
from tqdm import tqdm
from pydantic.dataclasses import dataclass

device = "cuda" # the device to load the model onto
attn_implementation = "sdpa"
model_name = "mistralai/Mistral-7B-Instruct-v0.2"

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = torch.float16, attn_implementation=attn_implementation)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# prompt = "Whos is king kohli?"

# encodeds = tokenizer([prompt], return_tensors="pt")
# model_inputs = encodeds.to(device)
# model.to(device)
# generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)
# decoded = tokenizer.batch_decode(generated_ids)


# print(decoded[0])


# The dataclass code is taken from the author's github repository.
T = TypeVar("T")

@dataclass(frozen=True)
class Document:
    title: str
    text: str
    id: Optional[str] = None
    score: Optional[float] = None
    hasanswer: Optional[bool] = None
    isgold: Optional[bool] = None
    original_retrieval_index: Optional[int] = None

    @classmethod
    def from_dict(cls: Type[T], data: dict) -> T:
        data = deepcopy(data)
        if not data:
            raise ValueError("Must provide data for creation of Document from dict.")
        id = data.pop("id", None)
        score = data.pop("score", None)
        # Convert score to float if it's provided.
        if score is not None:
            score = float(score)
        return cls(**dict(data, id=id, score=score))
    


def get_qa_prompt(question: str, documents: List[Document], query_aware_contextualization: bool):
    if query_aware_contextualization:
        prompt_path = "./prompting/qa_with_query_aware_contextualization.prompt"
    else:
        prompt_path = "./prompting/qa.prompt"
        
    with open(prompt_path) as f:
        prompt_template = f.read().rstrip("\n")

    # Format the documents into strings
    formatted_documents = []
    for document_index, document in enumerate(documents):
        formatted_documents.append(f"Document [{document_index+1}](Title: {document.title}) {document.text}")
    
    return prompt_template.format(question=question, search_results="\n".join(formatted_documents))




def get_multi_doc_qa_responses(inp : str, out : str, query_aware_contextualization: bool):
    prompts = []
    responses = []
    correct_responses = []
    durations = []
    
    with open(inp) as fin:
        for line in tqdm(fin):
            input_example = json.loads(line)
            
            # Getting question and correct answer
            question = input_example["question"]
            correct_answer = input_example["answers"]
            
            documents = []
            for ctx in deepcopy(input_example["ctxs"]):
                documents.append(Document.from_dict(ctx))
            
            qa_prompt = get_qa_prompt(question, documents, query_aware_contextualization)
            
            prompts.append(qa_prompt)
            correct_responses.append(correct_answer)

            encodeds = tokenizer([qa_prompt], return_tensors="pt")
            model_inputs = encodeds.to(device)
            model.to(device)
            start = time.process_time()
            generated_ids = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=100, do_sample=False)
            decoded = tokenizer.batch_decode(generated_ids)
            durations.append(time.process_time() - start)
            
            responses.append(decoded[0])
             
            
    with open(out, "w") as f:
        for prompt, response, correct_answer, duration in zip(prompts, responses, correct_responses, durations):
            output = {}

            output["model_prompt"] = prompt
            output["model_answer"] = response
            output["model"] = model_name
            output["correct_answer"] = correct_answer
            output["time"] = duration

            f.write(json.dumps(output) + "\n")

input_paths = [
    "./qa_data/10_total_documents/nq-open-10_total_documents_gold_at_0.jsonl",
    "./qa_data/10_total_documents/nq-open-10_total_documents_gold_at_4.jsonl",
    "./qa_data/10_total_documents/nq-open-10_total_documents_gold_at_9.jsonl",]

output_paths = ["./mist_7b_sdpa_10_doc_at_0_responses.jsonl",
               "./mist_7b_sdpa_10_doc_at_4_responses.jsonl",
               "./mist_7b_sdpa_10_doc_at_9_responses.jsonl",]


for i in range(len(input_paths)):
    get_multi_doc_qa_responses(input_paths[i], output_paths[i], False)


output_paths = ["./mist_7b_sdpa_10_doc_at_0_QAC_responses.jsonl",
               "./mist_7b_sdpa_10_doc_at_4_QAC_responses.jsonl",
               "./mist_7b_sdpa_10_doc_at_9_QAC_responses.jsonl",]


for i in range(len(input_paths)):
    get_multi_doc_qa_responses(input_paths[i], output_paths[i], True)



input_paths = [
    "./qa_data/20_total_documents/nq-open-20_total_documents_gold_at_0.jsonl",
    "./qa_data/20_total_documents/nq-open-20_total_documents_gold_at_4.jsonl",
    "./qa_data/20_total_documents/nq-open-20_total_documents_gold_at_9.jsonl",
    "./qa_data/20_total_documents/nq-open-20_total_documents_gold_at_14.jsonl",
    "./qa_data/20_total_documents/nq-open-20_total_documents_gold_at_19.jsonl"]

output_paths = ["./mist_7b_sdpa_20_doc_at_0_responses.jsonl",
               "./mist_7b_sdpa_20_doc_at_4_responses.jsonl",
               "./mist_7b_sdpa_20_doc_at_9_responses.jsonl",
               "./mist_7b_sdpa_20_doc_at_14_responses.jsonl",
               "./mist_7b_sdpa_20_doc_at_19_responses.jsonl"]


for i in range(len(input_paths)):
    get_multi_doc_qa_responses(input_paths[i], output_paths[i], False)


output_paths = ["./mist_7b_sdpa_20_doc_at_0_QAC_responses.jsonl",
               "./mist_7b_sdpa_20_doc_at_4_QAC_responses.jsonl",
               "./mist_7b_sdpa_20_doc_at_9_QAC_responses.jsonl",
               "./mist_7b_sdpa_20_doc_at_14_QAC_responses.jsonl",
               "./mist_7b_sdpa_20_doc_at_19_QAC_responses.jsonl"]


for i in range(len(input_paths)):
    get_multi_doc_qa_responses(input_paths[i], output_paths[i], True)

input_paths = [
    "./qa_data/30_total_documents/nq-open-30_total_documents_gold_at_0.jsonl",
    "./qa_data/30_total_documents/nq-open-30_total_documents_gold_at_4.jsonl",
    "./qa_data/30_total_documents/nq-open-30_total_documents_gold_at_9.jsonl",
    "./qa_data/30_total_documents/nq-open-30_total_documents_gold_at_14.jsonl",
    "./qa_data/30_total_documents/nq-open-30_total_documents_gold_at_19.jsonl",
    "./qa_data/30_total_documents/nq-open-30_total_documents_gold_at_24.jsonl",
    "./qa_data/30_total_documents/nq-open-30_total_documents_gold_at_29.jsonl",
    ]

output_paths = ["./mist_7b_sdpa_30_doc_at_0_QAC_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_4_QAC_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_9_QAC_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_14_QAC_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_19_QAC_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_24_QAC_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_29_QAC_responses.jsonl",
               ]


for i in range(len(input_paths)):
    get_multi_doc_qa_responses(input_paths[i], output_paths[i], True)


output_paths = ["./mist_7b_sdpa_30_doc_at_0_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_4_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_9_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_14_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_19_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_24_responses.jsonl",
               "./mist_7b_sdpa_30_doc_at_29_responses.jsonl",
               ]


for i in range(len(input_paths)):
    get_multi_doc_qa_responses(input_paths[i], output_paths[i], False)